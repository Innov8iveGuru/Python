{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBf7NqtdQJKmpwoyRP2UTc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Innov8iveGuru/Python/blob/main/SentimentAnalysisNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Setup the environment and import all necessary libraries"
      ],
      "metadata": {
        "id": "0qslBuav90D7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7baP2Ly9xDJ",
        "outputId": "c4fbadbe-2fc9-4d5f-d69f-6f25e16f28ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Download NLTK data (if not already installed)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Load and IMDb dataset containing 50,000 reviews, labeled as either positive or negative."
      ],
      "metadata": {
        "id": "ieQtKqVz-bho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "# Load the dataset (only keeping the top 10,000 most frequent words)\n",
        "# x_train and x_test contain the reviews (encoded as word indices)\n",
        "# y_train and y_test contain the sentiment labels (1 = positive, 0 = negative)\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Let's explore the dataset\n",
        "print(f'Training data: {len(x_train)} reviews')\n",
        "print(f'Testing data: {len(x_test)} reviews')\n",
        "print(f'Example review (encoded): {x_train[0]}')\n",
        "print(f'Label (1 = positive, 0 = negative): {y_train[0]}')\n",
        "\n",
        "# Get the word index from IMDb to decode reviews back to words\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# Invert the index to get word->index mapping\n",
        "index_word = {i: word for word, i in word_index.items()}\n",
        "\n",
        "# Example: decoding the first review\n",
        "decoded_review = ' '.join([index_word.get(i - 3, '?') for i in x_train[0]])\n",
        "print(f'Decoded review: {decoded_review}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_w_7qSf5-5Mf",
        "outputId": "0bcfc151-a4c3-4f03-93b1-c4bda02964e9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Training data: 25000 reviews\n",
            "Testing data: 25000 reviews\n",
            "Example review (encoded): [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "Label (1 = positive, 0 = negative): 1\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Decoded review: ? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Data Preprocessing\n",
        "We’ll follow these steps for preprocessing:\n",
        "\n",
        "i) Tokenization: Convert text into individual words (tokens).\n",
        "\n",
        "ii) Stop Word Removal: Remove words that don’t add much meaning, such as \"the,\" \"is,\" etc.\n",
        "\n",
        "iii) Vectorization: Use techniques like Bag of Words or TF-IDF to convert the text into numerical features.\n",
        "\n",
        "Since the dataset is already tokenized (converted to indices), we can directly apply vectorization techniques like TF-IDF."
      ],
      "metadata": {
        "id": "nia_q2I3_bvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Decode the dataset to get the raw reviews (since they're currently word indices)\n",
        "def decode_review(encoded_review):\n",
        "    return ' '.join([index_word.get(i - 3, '?') for i in encoded_review])\n",
        "\n",
        "# Convert the training and test datasets into decoded form (text format)\n",
        "x_train_text = [' '.join([index_word.get(i - 3, '?') for i in review]) for review in x_train]\n",
        "x_test_text = [' '.join([index_word.get(i - 3, '?') for i in review]) for review in x_test]\n",
        "\n",
        "# Vectorize the data using TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000, stop_words='english')\n",
        "\n",
        "# Fit and transform the training data, and transform the test data\n",
        "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train_text)\n",
        "x_test_tfidf = tfidf_vectorizer.transform(x_test_text)\n",
        "\n",
        "# Check the shape of the transformed data\n",
        "print(f'Training data shape: {x_train_tfidf.shape}')\n",
        "print(f'Testing data shape: {x_test_tfidf.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QcSlx3w_09a",
        "outputId": "eb079079-cb64-4194-d3a1-8eb8fc7d41bd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (25000, 9477)\n",
            "Testing data shape: (25000, 9477)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Model Building (Naive Bayes)\n",
        "We will use Multinomial Naive Bayes from Scikit-Learn, which works well with word frequencies (like those produced by TF-IDF)."
      ],
      "metadata": {
        "id": "aCW65DRFAWoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "\n",
        "# Initialize the Multinomial Naive Bayes model\n",
        "nb_model = MultinomialNB()\n",
        "\n",
        "# Train the model on the training data\n",
        "nb_model.fit(x_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = nb_model.predict(x_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Naive Bayes Model Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print a classification report\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeKmvdsGAcND",
        "outputId": "9ec2c9b2-2eda-4e3c-bb8f-c0070ce491c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Model Accuracy: 83.64%\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.87      0.84     12500\n",
            "           1       0.86      0.81      0.83     12500\n",
            "\n",
            "    accuracy                           0.84     25000\n",
            "   macro avg       0.84      0.84      0.84     25000\n",
            "weighted avg       0.84      0.84      0.84     25000\n",
            "\n",
            "Confusion Matrix:\n",
            " [[10846  1654]\n",
            " [ 2437 10063]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to improve the model accuracy, we will turn to using logistic regression for the same task."
      ],
      "metadata": {
        "id": "ev_VT46lAnLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize Logistic Regression model\n",
        "lr_model = LogisticRegression(max_iter=200)  # Increasing max_iter for convergence\n",
        "\n",
        "# Train the model on the training data\n",
        "lr_model.fit(x_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_lr = lr_model.predict(x_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_lr = accuracy_score(y_test, y_pred_lr)\n",
        "print(f'Logistic Regression Model Accuracy: {accuracy_lr * 100:.2f}%')\n",
        "\n",
        "# Print a classification report\n",
        "print(\"Classification Report (Logistic Regression):\\n\", classification_report(y_test, y_pred_lr))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix (Logistic Regression):\\n\", confusion_matrix(y_test, y_pred_lr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtIplvQ0CnUV",
        "outputId": "71e7f78e-8bd8-4d2c-a2f6-ee948d18e36c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 88.12%\n",
            "Classification Report (Logistic Regression):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88     12500\n",
            "           1       0.88      0.88      0.88     12500\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.88     25000\n",
            "weighted avg       0.88      0.88      0.88     25000\n",
            "\n",
            "Confusion Matrix (Logistic Regression):\n",
            " [[10996  1504]\n",
            " [ 1466 11034]]\n"
          ]
        }
      ]
    }
  ]
}